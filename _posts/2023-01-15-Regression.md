---
layout: post
title:  "[Machine Learning] Linear Regression"

categories : Machine_Learning
  
tags:
  - Machine Learning
  - Deep Learning
  - AI
  - Tensorflow
  - Python
---



머신러닝과 딥러닝에 대해 공부해보려고 합니다   
모두를 위한 머신러닝/딥러닝 강의를 수강하고 배운 것들을 정리해보겠습니다   
[참고] (https://hunkim.github.io/ml/)   

## Linear Regression
### Regression 
- Training Data

    |x|y|
    |--|--|
    |1|1|
    |2|2|
    |3|3|
  
* * *   

### Linear Hypothesis 
우리의 데이터들과 맞아 떨어지는 어떠한 **선형적인 모델**
> H(x)= Wx + b     

- linear model 을 통한 가설을 세운다

- 어떤 방정식이 더 가설과 잘 맞을까 ?
가장 적합한 W와 b값을 구해야함! -> 더 좋은 모델    

* * *

### Cost Function
= Loss function   
- 우리가 세운 Linear Hypothesis와 실제 데이터가 얼마나 다른가?
>       H(x)-y      -> 음수가 나올수도 있기 때문에 좋은 식은 아니다   
>       (|H(x)-y|)^2

![image.png](attachment:image.png)
m = 학습 데이터의 개수   
cost = 예측한 값과 실제 값의 차이의 제곱을 다 더한 후 학습 데이터의 개수로 나누어준다   

- Minimize Cost
> Linear Regression의 목표   
> -> Cost의 값이 가장 작게 하도록 W와 b를 설정한다!


## TensorFlow로 구현하기

### 1) Build graph using TF operations

H(x) = Wx + b 의 선형식에서 W와 b 구하기
tensorFlow를 통한 regression graph 구현하기

- Variable : tensorflow가 변경시키는 값 (trainable variable)
- 만드는 법 
    - shape에 랜덤한 값을 넣어준다   
 tf.random_normal([1]) : rank가 1이고 값이 1인 array

    - 이름 선언


```python
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

# X and Y data가 주어짐
x_train = [1,2,3]
y_train = [1,2,3]


W = tf.Variable(tf.random.normal([1]), name="weight")
b = tf.Variable(tf.random.normal([1]), name="bias")
# Our hypothesis XW + b
hypothesis = x_train * W + b

# cost/Loss function
# tf.reduce_mean -> tensor들의 평균값을 반환해준다
cost = tf.reduce_mean(tf.square(hypothesis - y_train))

#Minimize -> GradientDescent 이용
optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)
train = optimizer.minimize(cost)
print(train)
```

    name: "GradientDescent_6"
    op: "NoOp"
    input: "^GradientDescent_6/update_weight_6/ApplyGradientDescent"
    input: "^GradientDescent_6/update_bias_6/ApplyGradientDescent"
    


### 2) Run/update graph and get results
regression을 실행하기 위한 session을 만들고 W,b와 같은 Variable을 그래프에 global Variable로 초기화 해줘야한다





```python
# Launch the graph in a session
sess = tf.Session()
# Initializes global variables in the graph
sess.run(tf.global_variables_initializer())

# Fit the line
for step in range(2001):
    sess.run(train)
    # 2001번 출력은 너무 많으니까 줄임
    if step % 20  == 0:
         print(step, sess.run(cost), sess.run(W),sess.run(b) )
```

    0 3.1567323 [-0.51988447] [1.7682738]
    20 0.56634915 [0.08483654] [1.9197675]
    40 0.49350792 [0.17862557] [1.8518835]
    60 0.44802266 [0.22206235] [1.7669789]
    80 0.4068997 [0.25908297] [1.6841397]


    2023-01-15 22:15:43.107442: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
    2023-01-15 22:15:43.107458: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
    2023-01-15 22:15:43.128914: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.
    2023-01-15 22:15:43.160913: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.
    2023-01-15 22:15:43.177719: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.
    2023-01-15 22:15:43.187553: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.
    2023-01-15 22:15:43.189477: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.


    100 0.3695528 [0.29394722] [1.6050106]
    120 0.33563364 [0.32713333] [1.5295829]
    140 0.3048279 [0.358756] [1.4576982]
    160 0.27684963 [0.3888921] [1.3891919]
    180 0.25143915 [0.41761196] [1.3239048]
    200 0.22836113 [0.44498208] [1.2616863]
    220 0.20740119 [0.47106585] [1.2023915]
    240 0.18836503 [0.49592388] [1.1458834]
    260 0.17107612 [0.5196135] [1.0920311]
    280 0.15537404 [0.54219013] [1.0407094]
    300 0.14111316 [0.5637055] [0.99179995]
    320 0.12816125 [0.5842097] [0.9451891]
    340 0.11639807 [0.6037504] [0.90076864]
    360 0.105714634 [0.6223726] [0.8584358]
    380 0.09601171 [0.64011973] [0.8180924]
    400 0.08719935 [0.6570328] [0.77964514]
    420 0.07919585 [0.673151] [0.7430047]
    440 0.071926944 [0.68851167] [0.70808625]
    460 0.06532521 [0.7031504] [0.6748089]
    480 0.05932939 [0.7171012] [0.6430954]
    500 0.05388398 [0.73039645] [0.6128724]
    520 0.048938334 [0.74306667] [0.5840698]
    540 0.04444652 [0.75514156] [0.5566206]
    560 0.04036708 [0.76664907] [0.5304616]
    580 0.03666204 [0.77761567] [0.5055319]
    600 0.033297013 [0.788067] [0.48177364]
    620 0.030240905 [0.798027] [0.4591321]
    640 0.027465252 [0.807519] [0.43755457]
    660 0.0249444 [0.816565] [0.4169911]
    680 0.022654904 [0.8251857] [0.39739403]
    700 0.020575542 [0.83340126] [0.378718]
    720 0.018687043 [0.84123087] [0.36091965]
    740 0.016971875 [0.84869236] [0.3439578]
    760 0.015414117 [0.8558033] [0.32779303]
    780 0.013999347 [0.86258] [0.3123879]
    800 0.012714429 [0.8690383] [0.29770675]
    820 0.011547448 [0.87519306] [0.2837156]
    840 0.010487573 [0.8810585] [0.27038202]
    860 0.009524971 [0.88664836] [0.25767505]
    880 0.008650739 [0.89197546] [0.24556524]
    900 0.007856743 [0.8970521] [0.23402457]
    920 0.007135628 [0.9018903] [0.22302635]
    940 0.006480689 [0.9065011] [0.21254495]
    960 0.005885869 [0.9108952] [0.2025561]
    980 0.0053456347 [0.9150828] [0.19303672]
    1000 0.004854993 [0.9190736] [0.18396474]
    1020 0.00440938 [0.92287683] [0.17531908]
    1040 0.0040046736 [0.92650133] [0.16707975]
    1060 0.0036371003 [0.92995554] [0.1592276]
    1080 0.0033032768 [0.9332473] [0.1517445]
    1100 0.0030000904 [0.93638444] [0.14461307]
    1120 0.002724728 [0.93937427] [0.13781673]
    1140 0.002474643 [0.94222337] [0.13133985]
    1160 0.0022475102 [0.94493866] [0.12516741]
    1180 0.0020412267 [0.94752634] [0.119285]
    1200 0.0018538721 [0.9499924] [0.11367908]
    1220 0.0016837176 [0.95234257] [0.10833656]
    1240 0.0015291786 [0.9545823] [0.10324515]
    1260 0.0013888249 [0.9567168] [0.09839299]
    1280 0.0012613521 [0.95875096] [0.09376887]
    1300 0.001145581 [0.96068954] [0.08936202]
    1320 0.0010404333 [0.962537] [0.08516233]
    1340 0.0009449364 [0.96429753] [0.08115999]
    1360 0.00085820555 [0.9659755] [0.07734577]
    1380 0.000779439 [0.96757454] [0.07371078]
    1400 0.0007078991 [0.9690983] [0.07024665]
    1420 0.0006429225 [0.97055066] [0.06694533]
    1440 0.0005839157 [0.9719347] [0.06379914]
    1460 0.0005303215 [0.9732536] [0.06080081]
    1480 0.00048164706 [0.9745106] [0.05794338]
    1500 0.00043743692 [0.9757085] [0.05522025]
    1520 0.0003972867 [0.9768501] [0.05262512]
    1540 0.00036082423 [0.97793806] [0.05015195]
    1560 0.0003277046 [0.9789749] [0.04779499]
    1580 0.00029762666 [0.9799631] [0.04554875]
    1600 0.00027030878 [0.98090476] [0.04340809]
    1620 0.00024550012 [0.98180217] [0.04136806]
    1640 0.00022296555 [0.9826574] [0.03942388]
    1660 0.00020250019 [0.9834724] [0.03757111]
    1680 0.00018391378 [0.9842491] [0.0358054]
    1700 0.00016703442 [0.9849894] [0.03412266]
    1720 0.00015170308 [0.9856948] [0.03251903]
    1740 0.00013777928 [0.9863671] [0.03099075]
    1760 0.00012513302 [0.9870078] [0.02953429]
    1780 0.00011364912 [0.9876184] [0.02814631]
    1800 0.0001032162 [0.9882003] [0.02682354]
    1820 9.3741925e-05 [0.98875487] [0.02556291]
    1840 8.5139145e-05 [0.9892832] [0.02436158]
    1860 7.732539e-05 [0.98978686] [0.02321676]
    1880 7.022765e-05 [0.99026686] [0.02212569]
    1900 6.3782165e-05 [0.9907244] [0.02108582]
    1920 5.79286e-05 [0.9911603] [0.02009483]
    1940 5.261124e-05 [0.9915757] [0.01915044]
    1960 4.778182e-05 [0.9919716] [0.01825043]
    1980 4.3396896e-05 [0.9923489] [0.01739273]
    2000 3.941264e-05 [0.9927085] [0.01657531]


- Placeholder
: x, y data를 직접 값을 주지 않고 Placeholder를 사용하여 필요할때 노드를 주고 필요할 때 마다 값을 던져준다 


```python
X = tf.placeholder(tf.float32, shape=[None])
Y = tf.placeholder(tf.float32, shape=[None])

for step in range(2001):
    cost_val, W_val, b_val, _ = sess.run([cost,W,b,train], 
        feed_dict = {X:[1,2,3,4,5], Y:[2.1,3.1,4.1,5.1,6.1]})
    if step % 20 ==0 :
        print(step, cost_val, W_val, b_val)
```

    0 3.941264e-05 [0.992726] [0.01653547]
    20 3.579502e-05 [0.99306786] [0.01575835]
    40 3.2509713e-05 [0.99339366] [0.01501776]
    60 2.952654e-05 [0.9937042] [0.01431197]
    80 2.681687e-05 [0.994] [0.01363935]


    2023-01-15 22:15:45.243245: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.


    100 2.4354831e-05 [0.994282] [0.01299835]
    120 2.2119491e-05 [0.9945507] [0.01238749]
    140 2.008957e-05 [0.9948068] [0.01180531]
    160 1.8245053e-05 [0.9950509] [0.0112505]
    180 1.6570753e-05 [0.9952835] [0.01072175]
    200 1.50501855e-05 [0.99550515] [0.01021788]
    220 1.3668358e-05 [0.9957164] [0.00973767]
    240 1.2414077e-05 [0.9959177] [0.00928003]
    260 1.1274696e-05 [0.99610955] [0.00884391]
    280 1.0239557e-05 [0.9962924] [0.00842827]
    300 9.299448e-06 [0.9964667] [0.00803216]
    320 8.446204e-06 [0.9966327] [0.00765468]
    340 7.670927e-06 [0.99679095] [0.00729494]
    360 6.967037e-06 [0.99694175] [0.00695212]
    380 6.327589e-06 [0.99708545] [0.00662542]
    400 5.746848e-06 [0.9972224] [0.00631405]
    420 5.219524e-06 [0.99735296] [0.00601734]
    440 4.7404587e-06 [0.99747735] [0.00573457]
    460 4.3052523e-06 [0.9975959] [0.00546505]
    480 3.910025e-06 [0.9977089] [0.00520823]
    500 3.5513247e-06 [0.99781656] [0.00496348]
    520 3.2251742e-06 [0.99791914] [0.0047302]
    540 2.9291339e-06 [0.99801695] [0.00450792]
    560 2.6603861e-06 [0.9981102] [0.00429606]
    580 2.4162937e-06 [0.998199] [0.00409414]
    600 2.1945e-06 [0.9982836] [0.00390173]
    620 1.993062e-06 [0.99836427] [0.00371836]
    640 1.8100931e-06 [0.9984411] [0.00354363]
    660 1.643993e-06 [0.99851435] [0.00337711]
    680 1.4932397e-06 [0.9985842] [0.00321841]
    700 1.3560018e-06 [0.9986507] [0.00306715]
    720 1.2316291e-06 [0.99871415] [0.00292303]
    740 1.1186992e-06 [0.9987746] [0.00278567]
    760 1.0159301e-06 [0.9988321] [0.00265478]
    780 9.227174e-07 [0.998887] [0.00253004]
    800 8.380101e-07 [0.99893934] [0.00241115]
    820 7.61227e-07 [0.9989891] [0.00229786]
    840 6.91325e-07 [0.9990366] [0.00218991]
    860 6.2787086e-07 [0.99908185] [0.00208701]
    880 5.7033196e-07 [0.99912506] [0.00198895]
    900 5.1795877e-07 [0.99916613] [0.0018955]
    920 4.7036127e-07 [0.9992053] [0.00180643]
    940 4.2729275e-07 [0.99924266] [0.00172157]
    960 3.8805692e-07 [0.9992782] [0.0016407]
    980 3.5250181e-07 [0.9993121] [0.00156363]
    1000 3.2013472e-07 [0.99934447] [0.00149017]
    1020 2.907451e-07 [0.99937516] [0.00142019]
    1040 2.6405394e-07 [0.9994045] [0.0013535]
    1060 2.398874e-07 [0.9994325] [0.00128993]
    1080 2.1782739e-07 [0.99945915] [0.00122934]
    1100 1.9786017e-07 [0.99948454] [0.0011716]
    1120 1.7967709e-07 [0.99950874] [0.00111658]
    1140 1.6326422e-07 [0.9995318] [0.00106414]
    1160 1.4825716e-07 [0.99955374] [0.00101417]
    1180 1.3467476e-07 [0.99957466] [0.00096656]
    1200 1.2233708e-07 [0.99959475] [0.0009212]
    1220 1.1111168e-07 [0.9996138] [0.00087795]
    1240 1.0093231e-07 [0.9996318] [0.00083674]
    1260 9.166833e-08 [0.9996491] [0.00079744]
    1280 8.325661e-08 [0.9996657] [0.00076001]
    1300 7.565064e-08 [0.99968123] [0.00072436]
    1320 6.8690255e-08 [0.99969625] [0.00069035]
    1340 6.242164e-08 [0.99971056] [0.00065797]
    1360 5.672058e-08 [0.99972403] [0.00062708]
    1380 5.1482502e-08 [0.99973714] [0.00059767]
    1400 4.6773803e-08 [0.99974924] [0.00056963]
    1420 4.249118e-08 [0.99976116] [0.00054296]
    1440 3.857889e-08 [0.99977225] [0.00051747]
    1460 3.5090945e-08 [0.999783] [0.00049324]
    1480 3.185506e-08 [0.9997931] [0.00047005]
    1500 2.8941093e-08 [0.9998027] [0.00044808]
    1520 2.6281006e-08 [0.9998121] [0.00042706]
    1540 2.3896785e-08 [0.9998208] [0.00040702]
    1560 2.1717408e-08 [0.9998292] [0.00038802]
    1580 1.9698632e-08 [0.99983734] [0.0003698]
    1600 1.7906137e-08 [0.99984485] [0.00035244]
    1620 1.6280529e-08 [0.999852] [0.00033602]
    1640 1.4787139e-08 [0.99985915] [0.0003203]
    1660 1.3445326e-08 [0.9998657] [0.00030522]
    1680 1.2208171e-08 [0.99987185] [0.00029096]
    1700 1.1097773e-08 [0.9998778] [0.00027742]
    1720 1.00688835e-08 [0.9998838] [0.00026443]
    1740 9.157328e-09 [0.9998892] [0.00025195]
    1760 8.310527e-09 [0.9998942] [0.00024016]
    1780 7.561658e-09 [0.9998991] [0.00022899]
    1800 6.884136e-09 [0.99990386] [0.00021835]
    1820 6.2464713e-09 [0.99990857] [0.00020809]
    1840 5.662457e-09 [0.99991274] [0.00019826]
    1860 5.1467977e-09 [0.9999168] [0.00018897]
    1880 4.679886e-09 [0.9999206] [0.00018017]
    1900 4.2669894e-09 [0.99992424] [0.00017183]
    1920 3.8686734e-09 [0.9999278] [0.00016387]
    1940 3.5149128e-09 [0.9999314] [0.00015617]
    1960 3.1922365e-09 [0.9999346] [0.00014876]
    1980 2.8959828e-09 [0.99993765] [0.00014174]
    2000 2.6313198e-09 [0.9999405] [0.00013509]

